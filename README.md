# hand-gesture
To demonstrate communication or interaction with the Deaf and Mute public is of utter extent nowadays.
Gestures are the physical movement form enacted by an individual to transmit some significant 
information.Here,we use two algorithms Convolutional Neural Network (CNN) and MobileNetv2 where 
the input images were pre-processed and given as input to the respective algorithms.By combining the 
output from all layer an map is produced so called an image that is compared with dataset and output is 
predicted. With the help of Serial Communicator CP 2102, the data is given to NODE MCU.The 
Controller is already interfaced with Arduino codes.As NODE MCU has the Wi-fi capability, it is paired 
with mobile through BLYNK app for text.The voice module APR33A1 is implemented to deliver the 
speech. It also rings the buzzer in case of emergency. . Our model is also designed in such a way to alert 
both indoor and outdoor in case of emergency. For indoor, we have designed buzzer to alert the people 
inside home. For Outdoor, they will receive messages through Blynk app. By implementing this, we can 
take care of them and do our works as well.
Recognition of gestures is shown below:
![gestures](https://user-images.githubusercontent.com/93436862/233553656-191f264f-4970-41d6-b62c-915d416eb9fb.jpeg)

